{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# from custom files\n",
    "from dataset import CompCarsImageFolder, WrapperDataset, ImagesFromTextFile\n",
    "from resnet import ResNet, resnet_cfg\n",
    "from resnet import train, validate\n",
    "from utils import fix_all_seeds, train_val_dataset, compute_mean_std_from_dataset, show_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Set root to the image folder of CompCars dataset ######\n",
    "\n",
    "### NOTE: ADAPT TO YOUR FOLDER STRUCTURE\n",
    "## EDO'S PATHS\n",
    "# root = '/Volumes/EDO/NNDL/CompCars dataset/data/image/'\n",
    "# train_file = '/Volumes/EDO/NNDL/CompCars dataset/data/train_test_split/classification/train.txt'\n",
    "# test_file = '/Volumes/EDO/NNDL/CompCars dataset/data/train_test_split/classification/test.txt'\n",
    "\n",
    "## MICHAEL'S PATHS\n",
    "root = '../cars_data/data/image/'\n",
    "train_file = '../cars_data/data/train_test_split/classification/train.txt'\n",
    "test_file = '../cars_data/data/train_test_split/classification/test.txt'\n",
    "\n",
    "#############################################################\n",
    "\n",
    "### Hyperparam configuration\n",
    "\n",
    "resnet_type = 'resnet18'                # 'resnet18', 'resnet34', 'resnet50'    \n",
    "\n",
    "params = {                              ## Training Params (inspired from original resnet paper: https://arxiv.org/pdf/1512.03385)\n",
    "    'epoch_num': 50,                    # number of epochs\n",
    "    'lr': 1e-1,                         # (initial) Learning Rate\n",
    "    'weight_decay': 1e-4,               # L2 Penalty\n",
    "    'batch_size': 128,                  # batch size (tune depending on hardware)\n",
    "    'momentum': 0.9,\n",
    "    \n",
    "    'hierarchy': 0,                     # Choose 0 for manufacturer classification, 1 for model classification\n",
    "    'val_split': 10000,                 # (float) Fraction of validation holdout set / (int) Absolute number of data points in holdout set\n",
    "    \n",
    "    'resnet': resnet_cfg[resnet_type],  # ResNet configuration\n",
    "\n",
    "    'use_train_test_split': False,      # True: use prepared split, False: use total dataset\n",
    "\n",
    "    'seed': 28,                         # for reproducibility (NOTE: may be still non-deterministic for multithreaded/multiprocess stuff, e.g. DataLoader)\n",
    "}\n",
    "# !!! NOTE: REMEMBER TO PASS SEED TO train_val_dataset FUNCTION AS ARGUMENT !!! \n",
    "fix_all_seeds(seed=params['seed'])\n",
    "\n",
    "### Device\n",
    "if torch.cuda.is_available():\n",
    "    params[\"device\"] = torch.device(\"cuda\")   # option for NVIDIA GPUs\n",
    "elif torch.backends.mps.is_available():\n",
    "    params[\"device\"] = torch.device(\"mps\")    # option for Mac M-series chips (GPUs)\n",
    "else:\n",
    "    params[\"device\"] = torch.device(\"cpu\")    # default option if none of the above devices are available\n",
    "\n",
    "print(\"Device: {}\".format(params[\"device\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline (prepared train-test-split)\n",
    "\n",
    "## Read train/test text files separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load prepared train_test_split\n",
    "train_set = ImagesFromTextFile(root, txt_file=train_file, hierarchy=params['hierarchy'])\n",
    "print(train_set.classes)\n",
    "print(len(train_set.classes))\n",
    "\n",
    "test_set = ImagesFromTextFile(root, txt_file=test_file, hierarchy=params['hierarchy'])\n",
    "print(test_set.classes)\n",
    "print(len(test_set.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the train dataset\n",
    "label_dict = {y: x for x, y in train_set.class_to_idx.items()}\n",
    "\n",
    "num_images_to_show = 15\n",
    "data_idx = np.random.randint(0, high=len(train_set), size=num_images_to_show)\n",
    "num_cols = 5\n",
    "num_rows = num_images_to_show // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))\n",
    "axes = axes.flatten()\n",
    "for i in range(num_images_to_show):\n",
    "    image, label = train_set[data_idx[i]]\n",
    "    np_img = np.array(image)\n",
    "    axes[i].imshow(np_img)\n",
    "    axes[i].set_title(label_dict[label])\n",
    "    axes[i].axis('off')\n",
    "    print(f\"Image shape: {image.size}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline (full data set)\n",
    "\n",
    "## Read total Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load full dataset\n",
    "# hierarchy=0 -> manufacturer classification; hierarchy=1 -> model classification\n",
    "total_set = CompCarsImageFolder(root, hierarchy=params['hierarchy'])\n",
    "print(total_set.classes)\n",
    "print(len(total_set.classes))\n",
    "\n",
    "### Train-Validation Split (returns Subset objects)\n",
    "######### !!! NOTE: PASS SEED TO KEEP TRAIN/VALIDATION SPLIT THE SAME !!! ################\n",
    "datasets = train_val_dataset(total_set, val_split=params['val_split'], seed=params['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the dataset\n",
    "label_dict = {y: x for x, y in total_set.class_to_idx.items()}\n",
    "\n",
    "num_images_to_show = 15\n",
    "data_idx = np.random.randint(0, high=len(total_set), size=num_images_to_show)\n",
    "num_cols = 5\n",
    "num_rows = num_images_to_show // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))\n",
    "axes = axes.flatten()\n",
    "for i in range(num_images_to_show):\n",
    "    image, label = total_set[data_idx[i]]\n",
    "    np_img = np.array(image)\n",
    "    axes[i].imshow(np_img)\n",
    "    axes[i].set_title(label_dict[label])\n",
    "    axes[i].axis('off')\n",
    "    print(f\"Image shape: {image.size}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute normalization statistics\n",
    "\n",
    "### TODO: recompute normalization for chosen seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE: This cell takes some time. Could be accelerated by:\n",
    "    1. Using dataloader (vectorized batches)\n",
    "    2. Resize images before computing statistics\n",
    "    3. Use fixed values computed once (with fixed seed) TODO \n",
    "'''\n",
    "\n",
    "# TODO: Recompute once with fixed seed\n",
    "if params['use_train_test_split']:\n",
    "    # resulting values from the lines below\n",
    "    train_mean, train_std = [0.4913, 0.4796, 0.4696], [0.2873, 0.2860, 0.2914]\n",
    "    val_mean, val_std = [0.4919, 0.4797, 0.4693], [0.2879, 0.2869, 0.2923]\n",
    "\n",
    "    # Compute mean and std for training dataset\n",
    "    # train_mean, train_std = compute_mean_std_from_dataset(train_set)\n",
    "    # print(f\"Training dataset mean: {train_mean}\")\n",
    "    # print(f\"Training dataset std: {train_std}\")\n",
    "\n",
    "    # Compute mean and std for validation dataset\n",
    "    # val_mean, val_std = compute_mean_std_from_dataset(test_set)\n",
    "    # print(f\"Validation dataset mean: {val_mean}\")\n",
    "    # print(f\"Validation dataset std: {val_std}\")\n",
    "    \n",
    "else:\n",
    "    # default (computed statistic on whole dataset)\n",
    "    mean, std = [0.483, 0.471, 0.463], [0.297, 0.296, 0.302]\n",
    "\n",
    "    # Compute mean and std for training dataset\n",
    "    # train_mean, train_std = compute_mean_std_from_dataset(datasets['train'])\n",
    "    # print(f\"Training dataset mean: {train_mean}\")\n",
    "    # print(f\"Training dataset std: {train_std}\")\n",
    "    train_mean, train_std = mean, std\n",
    "\n",
    "    # Compute mean and std for validation dataset\n",
    "    # val_mean, val_std = compute_mean_std_from_dataset(datasets['val'])\n",
    "    # print(f\"Validation dataset mean: {val_mean}\")\n",
    "    # print(f\"Validation dataset std: {val_std}\")\n",
    "    val_mean, val_std = mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data - Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation\n",
    "########################## Transforms ############################\n",
    "# TODO: maybe use v2 transforms: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "# Augmentation Strategy similar to original ResNet paper: https://arxiv.org/pdf/1512.03385\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomChoice([       # Scale Augmentation\n",
    "            transforms.Resize(256),\n",
    "            transforms.Resize(224),\n",
    "            transforms.Resize(320)\n",
    "        ]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(224),     # ResNet expects 224x224 images\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(train_mean, train_std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),     # Evaluate using 224x224 central part of image\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(val_mean, val_std)\n",
    "    ])\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "# Wrapper dataset used to make reading/splitting dataset independent from applying transform\n",
    "if params['use_train_test_split']:\n",
    "    wrapped_datasets = {\n",
    "        'train': WrapperDataset(train_set, transform=data_transforms['train']),\n",
    "        'val': WrapperDataset(test_set, transform=data_transforms['val'])\n",
    "    }\n",
    "else:\n",
    "    wrapped_datasets = {\n",
    "        'train': WrapperDataset(datasets['train'], transform=data_transforms['train']),\n",
    "        'val': WrapperDataset(datasets['val'], transform=data_transforms['val'])\n",
    "    }\n",
    "\n",
    "# NOTE: for num_workers != 0 dataloader won't be deterministic (for reproducible implementation see https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "dataloaders = {\n",
    "    'train': DataLoader(wrapped_datasets['train'], batch_size=params['batch_size'], shuffle=True, num_workers=4), # NOTE: num_workers to tune for performance or reproducability\n",
    "    'val': DataLoader(wrapped_datasets['val'], batch_size=params['batch_size'], shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Training dataset size: {len(wrapped_datasets['train'])}\")\n",
    "print(f\"Validation dataset size: {len(wrapped_datasets['val'])}\")\n",
    "\n",
    "x, y = next(iter(dataloaders['train']))\n",
    "print(f\"Batch of training images shape: {x.shape}\")\n",
    "print(f\"Batch of training labels shape: {y.shape}\")\n",
    "\n",
    "x, y = next(iter(dataloaders['val']))\n",
    "print(f\"Batch of validation images shape: {x.shape}\")\n",
    "print(f\"Batch of validation labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up resnet model\n",
    "if params['use_train_test_split']:\n",
    "    num_classes = len(train_set.classes)\n",
    "else:\n",
    "    num_classes = len(total_set.classes)\n",
    "\n",
    "resnet = ResNet(params['resnet']['block'], params['resnet']['layers'], \n",
    "            num_classes).to(params['device'])\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    resnet.parameters(), \n",
    "    lr=params['lr'], \n",
    "    weight_decay=params['weight_decay'], \n",
    "    momentum=params['momentum']\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     resnet.parameters(), \n",
    "#     lr=params['lr'], \n",
    "#     weight_decay=params['weight_decay']\n",
    "# )\n",
    "\n",
    "# LR scheduler (using Top-1-Accuracy as Validation metric)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, min_lr=1e-4, patience=2, threshold=1e-3)\n",
    "\n",
    "# Save performance metrics\n",
    "train_losses, validation_losses, train_acc, validation_acc, train_top5_acc, validation_top5_acc = list(), list(), list(), list(), list(), list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "To start from checkpoint, set `START_FROM_CHECKPOINT=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = './training_checkpoints/checkpoint.pth'\n",
    "START_FROM_CHECKPOINT = False\n",
    "start_epoch = 0\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    resnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    train_acc = checkpoint['train_acc']\n",
    "    train_top5_acc = checkpoint['train_top5_acc']\n",
    "    validation_losses = checkpoint['validation_losses']\n",
    "    validation_acc = checkpoint['validation_acc']\n",
    "    validation_top5_acc = checkpoint['validation_top5_acc']\n",
    "\n",
    "# Just some fancy progress bars\n",
    "pbar_epoch = trange(start_epoch, params[\"epoch_num\"], initial=start_epoch, total=params[\"epoch_num\"], desc=\"Training\", position=0, leave=True, unit=\"epoch\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}]\")\n",
    "pbar_inside_epoch = trange(0, (len(dataloaders['train'])+len(dataloaders['val'])), desc=\"Training and validation per epoch\", position=1, leave=False, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}]\")\n",
    "\n",
    "# Stop the training phase in case there is no improvement\n",
    "# early_stopper = EarlyStopper(patience=10, min_delta=0.1)\n",
    "\n",
    "for epoch in pbar_epoch:\n",
    "    pbar_inside_epoch.reset()\n",
    "\n",
    "    # Training\n",
    "    train_results = train(dataloaders['train'], resnet, epoch, criterion, optimizer, params[\"device\"], pbar=pbar_inside_epoch)\n",
    "    train_losses.append(train_results[0])\n",
    "    train_acc.append(1 - train_results[1])                 # saving acc error\n",
    "    train_top5_acc.append(1 - train_results[2])\n",
    "\n",
    "    # Validation\n",
    "    validation_results = validate(dataloaders['val'], resnet, epoch, criterion, params[\"device\"], pbar=pbar_inside_epoch)\n",
    "    validation_losses.append(validation_results[0])\n",
    "    validation_acc.append(1 - validation_results[1])       # saving acc error\n",
    "    validation_top5_acc.append(1 - validation_results[2])\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step(validation_results[1])   # ReduceLROnPlateau scheduler (reduce LR by 10 when top-1-accuracy does not improve)\n",
    "    print(\"\\nCurrent Learning Rate: \", round(scheduler.get_last_lr()[0], 4), \"\\n\")\n",
    "\n",
    "    # Checkpoint\n",
    "    torch.save({\n",
    "        'epoch' : epoch + 1,\n",
    "        'model_state_dict' : resnet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict' : scheduler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'train_acc': train_acc,\n",
    "        'train_top5_acc': train_top5_acc,\n",
    "        'validation_losses': validation_losses,\n",
    "        'validation_acc': validation_acc,\n",
    "        'validation_top5_acc': validation_top5_acc,\n",
    "    }, CHECKPOINT_PATH)\n",
    "\n",
    "    # Comment on the following lines if you don't want to stop early in case of no improvement\n",
    "    # if early_stopper.early_stop(validation_results[0]):\n",
    "    #     params['epoch_num'] = epoch\n",
    "    #     print(\"\\n\\nEarly stopping...\")\n",
    "    #     break\n",
    "\n",
    "pbar_inside_epoch.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the performance of the model in the training and validation phase\n",
    "\n",
    "plots = [\n",
    "    (np.arange(0, len(train_losses), 1), train_losses, \"Train Loss\"),\n",
    "    (np.arange(0, len(validation_losses), 1), validation_losses, \"Validation Loss\")\n",
    "]\n",
    "\n",
    "show_plot(plots, \"Model Loss for Epoch\", \"Epoch\", \"Loss\")\n",
    "\n",
    "plots = [\n",
    "    (np.arange(0, len(train_acc), 1), train_acc, \"Train Top-1-Error\"),\n",
    "    (np.arange(0, len(validation_acc), 1), validation_acc, \"Validation Top-1-Error\"),\n",
    "    (np.arange(0, len(train_top5_acc), 1), train_top5_acc, \"Train Top-5-Error\"),\n",
    "    (np.arange(0, len(validation_top5_acc), 1), validation_top5_acc, \"Validation Top-5-Error\")\n",
    "]\n",
    "\n",
    "show_plot(plots, \"Model Classification Error for Epoch\", \"Epoch\", \"Error Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = './trained_models/' + resnet_type + '_weights_car_'\n",
    "\n",
    "if params['hierarchy'] == 0:\n",
    "    MODEL_PATH += 'makers_'\n",
    "else:\n",
    "    MODEL_PATH += 'models_'\n",
    "    \n",
    "if params['use_train_test_split']:\n",
    "    MODEL_PATH += 'prepared_dataset_'\n",
    "else:\n",
    "    MODEL_PATH += 'full_dataset_'\n",
    "    \n",
    "MODEL_PATH += str(params['batch_size']) + '.pth'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict' : resnet.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_acc': train_acc,\n",
    "    'train_top5_acc': train_top5_acc,\n",
    "    'validation_losses': validation_losses,\n",
    "    'validation_acc': validation_acc,\n",
    "    'validation_top5_acc': validation_top5_acc,\n",
    "    'resnet': params['resnet'],\n",
    "    }, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Evaluate Saved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the saved model TO BE CHANGED\n",
    "MODEL_PATH = './trained_models/model.pth'\n",
    "saved_model = torch.load(MODEL_PATH, map_location=params['device'])\n",
    "\n",
    "if params['use_train_test_split']:\n",
    "    num_classes = len(train_set.classes)\n",
    "else:\n",
    "    num_classes = len(total_set.classes)\n",
    "\n",
    "resnet = ResNet(saved_model['resnet']['block'], saved_model['resnet']['layers'], \n",
    "                num_classes).to(params['device'])\n",
    "\n",
    "resnet.load_state_dict(saved_model['model_state_dict'])\n",
    "train_losses = saved_model['train_losses']\n",
    "train_acc = saved_model['train_acc']\n",
    "train_top5_acc = saved_model['train_top5_acc']\n",
    "validation_losses = saved_model['validation_losses']\n",
    "validation_acc = saved_model['validation_acc']\n",
    "validation_top5_acc = saved_model['validation_top5_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the performance of the model in the training and validation phase\n",
    "\n",
    "plots = [\n",
    "    (np.arange(0, len(train_losses), 1), train_losses, \"Train Loss\"),\n",
    "    (np.arange(0, len(validation_losses), 1), validation_losses, \"Validation Loss\")\n",
    "]\n",
    "\n",
    "show_plot(plots, \"Model Loss for Epoch\", \"Epoch\", \"Loss\")\n",
    "\n",
    "plots = [\n",
    "    (np.arange(0, len(train_acc), 1), train_acc, \"Train Top-1-Error\"),\n",
    "    (np.arange(0, len(validation_acc), 1), validation_acc, \"Validation Top-1-Error\"),\n",
    "    (np.arange(0, len(train_top5_acc), 1), train_top5_acc, \"Train Top-5-Error\"),\n",
    "    (np.arange(0, len(validation_top5_acc), 1), validation_top5_acc, \"Validation Top-5-Error\")\n",
    "]\n",
    "\n",
    "show_plot(plots, \"Model Classification Error for Epoch\", \"Epoch\", \"Error Rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

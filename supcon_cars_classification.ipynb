{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "d2a02713e64ac65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision import transforms\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "# from custom files\n",
    "from dataset import CompCarsImageFolder, WrapperDataset, ImagesFromTextFile, TwoCropTransform\n",
    "from models import ResNet, resnet_cfg, SupConResNet\n",
    "from models import train, validate\n",
    "from utils import *\n",
    "from losses import SupConLoss"
   ],
   "id": "82d4ea694eb16f09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "dd6bf6a5b4ce2a01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "###### Set root to the image folder of CompCars dataset ######\n",
    "\n",
    "#TODO: ADAPT TO YOUR FOLDER STRUCTURE\n",
    "\n",
    "# EDO'S PATHS\n",
    "root = '/Volumes/EDO/NNDL/CompCars dataset/data/image/'\n",
    "train_file = '/Volumes/EDO/NNDL/CompCars dataset/data/train_test_split/classification/train.txt'\n",
    "test_file = '/Volumes/EDO/NNDL/CompCars dataset/data/train_test_split/classification/test.txt'\n",
    "\n",
    "# DEFAULT PATHS\n",
    "#root = '../cars_data/data/image/'\n",
    "#train_file = '../cars_data/data/train_test_split/classification/train.txt'\n",
    "#test_file = '../cars_data/data/train_test_split/classification/test.txt'\n",
    "\n",
    "#############################################################\n",
    "\n",
    "### Hyperparam configuration # TODO: TUNE\n",
    "\n",
    "resnet_type = 'resnet18'                # 'resnet18', 'resnet34', 'resnet50'    \n",
    "\n",
    "params = {                              ## Training Params (inspired from original resnet paper: https://arxiv.org/pdf/1512.03385)\n",
    "    'epoch_num': 50,                    # number of epochs\n",
    "    'lr': 1e-1,                         # (initial) Learning Rate\n",
    "    'weight_decay': 1e-4,               # L2 Penalty\n",
    "    'batch_size': 512,                   # batch size (depends on hardware)\n",
    "    'momentum': 0.9,\n",
    "\n",
    "    'hierarchy': 0,                     # Choose 0 for manufacturer classification, 1 for model classification\n",
    "    'val_split': 10000,                 # (float) Fraction of validation holdout / (int) Absolute number of data points in holdout\n",
    "\n",
    "    'resnet': resnet_cfg[resnet_type],  # ResNet configuration\n",
    "\n",
    "    'use_train_test_split': False       # True: use prepared split, False: use total dataset\n",
    "}\n",
    "\n",
    "### Device\n",
    "if torch.cuda.is_available():\n",
    "    params[\"device\"] = torch.device(\"cuda\")   # option for NVIDIA GPUs\n",
    "elif torch.backends.mps.is_available():\n",
    "    params[\"device\"] = torch.device(\"mps\")    # option for Mac M-series chips (GPUs)\n",
    "else:\n",
    "    params[\"device\"] = torch.device(\"cpu\")    # default option if none of the above devices are available\n",
    "\n",
    "print(\"Device: {}\".format(params[\"device\"]))"
   ],
   "id": "64aeb465a10cc824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Pipeline (full data set)\n",
    "\n",
    "## Read total Dataset"
   ],
   "id": "4dd81382d6006547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Load full dataset\n",
    "# hierarchy=0 -> manufacturer classification; hierarchy=1 -> model classification\n",
    "total_set = CompCarsImageFolder(root, hierarchy=params['hierarchy'])\n",
    "print(total_set.classes)\n",
    "print(len(total_set.classes))"
   ],
   "id": "9472b23da173f2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Raw Data Visualization",
   "id": "d7ecb0893920afc3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization of the dataset\n",
    "label_dict = {y: x for x, y in total_set.class_to_idx.items()}\n",
    "\n",
    "num_images_to_show = 15\n",
    "data_idx = np.random.randint(0, high=len(total_set), size=num_images_to_show)\n",
    "num_cols = 5\n",
    "num_rows = num_images_to_show // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))\n",
    "axes = axes.flatten()\n",
    "for i in range(num_images_to_show):\n",
    "    image, label = total_set[data_idx[i]]\n",
    "    np_img = np.array(image)\n",
    "    axes[i].imshow(np_img)\n",
    "    axes[i].set_title(label_dict[label])\n",
    "    axes[i].axis('off')\n",
    "    print(f\"Image shape: {image.size}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "12fa820142552ef1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split in training and validation data",
   "id": "d982c78bcf94d847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets = train_val_dataset(total_set, val_split=params['val_split'])",
   "id": "fa29858e231659a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute normalization statistics",
   "id": "7eba8bf0cdce9ad4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "'''\n",
    "NOTE: This cell takes some time. Could be accelerated by:\n",
    "    1. Using dataloader (vectorized batches)\n",
    "    2. Resize images before computing statistics\n",
    "'''\n",
    "if params['use_train_test_split']:\n",
    "    # resulting values from the lines below\n",
    "    train_mean, train_std = [0.4913, 0.4796, 0.4696], [0.2873, 0.2860, 0.2914]\n",
    "    val_mean, val_std = [0.4919, 0.4797, 0.4693], [0.2879, 0.2869, 0.2923]\n",
    "\n",
    "    # Compute mean and std for training dataset\n",
    "    # train_mean, train_std = compute_mean_std_from_dataset(train_set)\n",
    "    # print(f\"Training dataset mean: {train_mean}\")\n",
    "    # print(f\"Training dataset std: {train_std}\")\n",
    "\n",
    "    # Compute mean and std for validation dataset\n",
    "    # val_mean, val_std = compute_mean_std_from_dataset(test_set)\n",
    "    # print(f\"Validation dataset mean: {val_mean}\")\n",
    "    # print(f\"Validation dataset std: {val_std}\")\n",
    "\n",
    "else:\n",
    "    # default (computed statistic on whole dataset)\n",
    "    mean, std = [0.483, 0.471, 0.463], [0.297, 0.296, 0.302]\n",
    "\n",
    "    # Compute mean and std for training dataset\n",
    "    # train_mean, train_std = compute_mean_std_from_dataset(datasets['train'])\n",
    "    # print(f\"Training dataset mean: {train_mean}\")\n",
    "    # print(f\"Training dataset std: {train_std}\")\n",
    "    train_mean, train_std = mean, std\n",
    "\n",
    "    # Compute mean and std for validation dataset\n",
    "    # val_mean, val_std = compute_mean_std_from_dataset(datasets['val'])\n",
    "    # print(f\"Validation dataset mean: {val_mean}\")\n",
    "    # print(f\"Validation dataset std: {val_std}\")\n",
    "    val_mean, val_std = mean, std"
   ],
   "id": "914884a9b6b065d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transform data - Prepare DataLoaders",
   "id": "9e498beae8085d48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply transformation\n",
    "########################## Transforms ############################\n",
    "# TODO: Adapt transforms to our data set\n",
    "# TODO: maybe use v2 transforms: https://pytorch.org/vision/stable/transforms.html\n",
    "\n",
    "# inspired from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "data_transforms = { # TODO: TUNE\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomChoice([\n",
    "            transforms.Resize(256),\n",
    "            transforms.Resize(224),\n",
    "            transforms.Resize(320)\n",
    "        ]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(train_mean, train_std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(val_mean, val_std)\n",
    "    ])\n",
    "}\n",
    "##################################################################\n",
    "\n",
    "wrapped_datasets = {\n",
    "    'train': WrapperDataset(datasets['train'], transform=TwoCropTransform(data_transforms['train'])),\n",
    "    'val': WrapperDataset(datasets['val'], transform=data_transforms['val'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(wrapped_datasets['train'], batch_size=params['batch_size'], shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(wrapped_datasets['val'], batch_size=params['batch_size'], shuffle=False, num_workers=4)\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"Training dataset size: {len(wrapped_datasets['train'])}\")\n",
    "print(f\"Validation dataset size: {len(wrapped_datasets['val'])}\")\n",
    "\n",
    "x, y = next(iter(dataloaders['train']))\n",
    "print(f\"Batch of training images shape: {x[0].shape}\")\n",
    "print(f\"Batch of training labels shape: {y.shape}\")\n",
    "\n",
    "x, y = next(iter(dataloaders['val']))\n",
    "print(f\"Batch of validation images shape: {x.shape}\")\n",
    "print(f\"Batch of validation labels shape: {y.shape}\")"
   ],
   "id": "6c24b750135bec81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training SupCon\n",
    "\n",
    "## Set-Up"
   ],
   "id": "df5d83520b7b4844"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up SupConResNet model\n",
    "sup_con_resnet = SupConResNet(len(total_set.classes), resnet_type=resnet_type).to(params['device'])\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = SupConLoss(params[\"device\"], temperature=0.1) # TODO: TUNE\n",
    "optimizer = torch.optim.SGD( #SGD used in original resnet paper # TODO: TUNE\n",
    "    sup_con_resnet.parameters(),\n",
    "    lr=params['lr'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "    momentum=params['momentum']\n",
    ")\n",
    "\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     resnet.parameters(), \n",
    "#     lr=params['lr'], \n",
    "#     weight_decay=params['weight_decay']\n",
    "# )\n",
    "\n",
    "# LR scheduler (using Top-1-Accuracy as Validation metric)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, min_lr=1e-4, patience=2, threshold=1e-3) # TODO: TUNE\n",
    "\n",
    "# Save performance metrics\n",
    "train_losses, validation_losses, train_acc, validation_acc, train_top5_acc, validation_top5_acc = list(), list(), list(), list(), list(), list()"
   ],
   "id": "58ee2081eb19f27c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n",
    "To start from checkpoint, set `START_FROM_CHECKPOINT=True`."
   ],
   "id": "f3ce74a2c7d8a8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CHECKPOINT_PATH = './training_checkpoints/checkpoint.pth'\n",
    "START_FROM_CHECKPOINT = False   # set to TRUE to start from checkpoint\n",
    "start_epoch = 0\n",
    "\n",
    "if START_FROM_CHECKPOINT:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    sup_con_resnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    train_losses = checkpoint['train_losses']\n",
    "\n",
    "# Just some fancy progress bars # FIXME: inside epoch progress bar not working reliably for me\n",
    "pbar_epoch = trange(start_epoch, params[\"epoch_num\"], initial=start_epoch, total=params[\"epoch_num\"], desc=\"Training\", position=0, leave=True, unit=\"epoch\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}]\")\n",
    "pbar_inside_epoch = trange(0, (len(dataloaders['train'])+len(dataloaders['val'])), desc=\"Training and validation per epoch\", position=1, leave=False, bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_inv_fmt}]\")\n",
    "\n",
    "# Stop the training phase in case there is no improvement\n",
    "# early_stopper = EarlyStopper(patience=10, min_delta=0.1)\n",
    "\n",
    "for epoch in pbar_epoch:\n",
    "    pbar_inside_epoch.reset()\n",
    "\n",
    "    # Training\n",
    "    train_results = train(dataloaders['train'], sup_con_resnet, epoch, criterion, optimizer, params[\"device\"], pbar=pbar_inside_epoch, sup_con=True)\n",
    "    train_losses.append(train_results[0])\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler.step(train_results[0])   # ReduceLROnPlateau scheduler (reduce LR by 10 when top-1-accuracy does not improve)\n",
    "    print(\"\\nCurrent Learning Rate: \", round(scheduler.get_last_lr()[0], 4), \"\\n\")\n",
    "\n",
    "    # Checkpoint\n",
    "    torch.save({\n",
    "        'epoch' : epoch + 1,\n",
    "        'model_state_dict' : sup_con_resnet.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict' : scheduler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "    }, CHECKPOINT_PATH)\n",
    "\n",
    "    # Comment on the following lines if you don't want to stop early in case of no improvement\n",
    "    # if early_stopper.early_stop(validation_results[0]):\n",
    "    #     params['epoch_num'] = epoch\n",
    "    #     print(\"\\n\\nEarly stopping...\")\n",
    "    #     break\n",
    "\n",
    "pbar_inside_epoch.close()"
   ],
   "id": "793e4cd160d2ebac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training Linear Classifier\n",
    "\n",
    "## Set-Up"
   ],
   "id": "bcbac1762149aee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
